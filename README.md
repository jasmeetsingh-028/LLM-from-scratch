# LLM-from-scratch 

[![Read Article](https://img.shields.io/badge/Read%20Article-Medium-blue)](https://medium.com/@sjasmeet135/transforming-text-generation-the-power-of-transformers-in-llms-703b236fa03b)

## Overview

## Bigram Language Model

The bigram language model is a simple statistical model that predicts the next word in a sequence based on the previous word. In this project, we implemented the bigram model using Pytorch and used Two Sailor Lads as the text corpus.

## GPT (Decoder-only Model)

Later in the project, we extended our modeling approach by implementing a GPT model from scratch. GPT, short for Generative Pre-trained Transformer, is a state-of-the-art neural network architecture for natural language processing tasks, including text generation.

Our implementation of GPT focuses on the decoder-only variant of the model, which means it only uses the decoder part of the Transformer architecture..
