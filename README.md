# LLM-from-scratch 

## Overview

## Bigram Language Model

The bigram language model is a simple statistical model that predicts the next word in a sequence based on the previous word. In this project, we implemented the bigram model using Pytorch and used Two Sailor Lads as the text corpus.

## GPT (Decoder-only Model)

Later in the project, we extended our modeling approach by implementing a GPT model from scratch. GPT, short for Generative Pre-trained Transformer, is a state-of-the-art neural network architecture for natural language processing tasks, including text generation.

Our implementation of GPT focuses on the decoder-only variant of the model, which means it only uses the decoder part of the Transformer architecture..
